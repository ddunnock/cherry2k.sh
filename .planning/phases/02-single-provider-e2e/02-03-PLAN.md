---
phase: 02-single-provider-e2e
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - Cargo.toml
  - crates/core/Cargo.toml
  - crates/core/src/provider/mod.rs
  - crates/core/src/provider/openai.rs
  - crates/core/src/provider/sse.rs
  - crates/core/src/lib.rs
  - crates/cli/Cargo.toml
  - crates/cli/src/main.rs
  - crates/cli/src/commands/chat.rs
  - crates/cli/src/signal.rs
autonomous: true

must_haves:
  truths:
    - "User can run `cherry2k chat \"What is Rust?\"` and receive a streamed response"
    - "Response streams to terminal line-by-line (not buffered until complete)"
    - "API errors surface as clear error messages (rate limit, invalid key, network)"
    - "User can cancel mid-stream with Ctrl+C (prompts for confirmation)"
  artifacts:
    - path: "crates/core/src/provider/openai.rs"
      provides: "OpenAI provider implementation with SSE streaming"
      exports: ["OpenAiProvider"]
    - path: "crates/core/src/provider/sse.rs"
      provides: "SSE parsing utilities for OpenAI format"
      exports: ["parse_sse_chunk"]
    - path: "crates/cli/src/signal.rs"
      provides: "Ctrl+C handling with confirmation"
      exports: ["setup_cancellation"]
    - path: "crates/cli/src/commands/chat.rs"
      provides: "Chat command with streaming display"
  key_links:
    - from: "crates/core/src/provider/openai.rs"
      to: "crates/core/src/provider/trait.rs"
      via: "impl AiProvider for OpenAiProvider"
      pattern: "impl AiProvider for OpenAiProvider"
    - from: "crates/cli/src/commands/chat.rs"
      to: "crates/core/src/provider/openai.rs"
      via: "OpenAiProvider::new() and complete()"
      pattern: "OpenAiProvider::new"
    - from: "crates/cli/src/commands/chat.rs"
      to: "crates/cli/src/output/*"
      via: "StreamWriter, ResponseSpinner usage"
      pattern: "(StreamWriter|ResponseSpinner)"

user_setup:
  - service: openai
    why: "AI provider for chat responses"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API Keys -> Create new secret key"
    dashboard_config: []
---

<objective>
Implement OpenAI provider with SSE streaming and integrate with chat command for end-to-end AI responses.

Purpose: This is the core deliverable of Phase 2 - proving the AI interaction flow works. User runs `cherry2k chat "prompt"` and sees streaming response with proper error handling and cancellation support.

Output: Working chat command that streams OpenAI responses with spinner, error handling, and Ctrl+C cancellation.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-single-provider-e2e/02-CONTEXT.md
@.planning/phases/02-single-provider-e2e/02-RESEARCH.md
@.planning/phases/02-single-provider-e2e/02-01-SUMMARY.md (once available)
@.planning/phases/02-single-provider-e2e/02-02-SUMMARY.md (once available)
@.planning/phases/01-foundation-and-safety/01-02-SUMMARY.md

Key constraints from CONTEXT.md:
- Ctrl+C prompts "Cancel response? [y/n]" before stopping (not immediate)
- Clean separation — blank line before and after response
- Subtle icon prefix before response

From RESEARCH.md:
- reqwest-eventsource 0.6 for SSE handling
- OpenAI sends data: [DONE] as completion signal
- tokio::select! for racing stream vs cancellation
- spawn_blocking for stdin confirmation (avoid blocking async)
- Parse defensively (.get(0).and_then()) for SSE chunks
- tokio::pin!(stream) before iteration

From 01-02-SUMMARY.md:
- Config has openai.api_key and openai.base_url
- Env var OPENAI_API_KEY overrides config file
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SSE dependencies and create OpenAI provider</name>
  <files>
    Cargo.toml
    crates/core/Cargo.toml
    crates/core/src/provider/mod.rs
    crates/core/src/provider/openai.rs
    crates/core/src/provider/sse.rs
    crates/core/src/lib.rs
  </files>
  <action>
Add to workspace Cargo.toml [workspace.dependencies]:
- reqwest-eventsource = "0.6"
- tokio-util = { version = "0.7", features = ["rt"] }

Add to crates/core/Cargo.toml dependencies:
- reqwest-eventsource.workspace = true
- tokio-util.workspace = true

Create crates/core/src/provider/sse.rs:
- Define OpenAiChunk struct for serde deserialization:
  ```rust
  #[derive(Debug, Deserialize)]
  pub struct OpenAiChunk {
      pub choices: Vec<OpenAiChoice>,
  }

  #[derive(Debug, Deserialize)]
  pub struct OpenAiChoice {
      pub delta: OpenAiDelta,
  }

  #[derive(Debug, Deserialize)]
  pub struct OpenAiDelta {
      pub content: Option<String>,
  }
  ```
- Implement parse_sse_chunk(data: &str) -> Option<String>:
  - If data == "[DONE]", return None
  - Parse JSON into OpenAiChunk
  - Extract content: choices.get(0).and_then(|c| c.delta.content.clone())
  - Log parse errors with tracing::warn, return None (don't break stream)

Create crates/core/src/provider/openai.rs:
- Define OpenAiProvider struct with client: reqwest::Client, config: OpenAiConfig
- Implement new(config: OpenAiConfig) -> Self
- Implement AiProvider trait:
  - provider_id() -> "openai"
  - validate_config() -> check api_key is Some and non-empty, return ConfigError::MissingField if not
  - health_check() -> HEAD request to base_url, return Unavailable on error
  - complete() ->
    - Build request body: model, messages, stream: true
    - POST to {base_url}/chat/completions with Authorization: Bearer {api_key}
    - Use reqwest-eventsource to get EventSource
    - Return stream using async_stream::try_stream! that:
      - Loops over EventSource events
      - On Message event, call parse_sse_chunk
      - Yield content if Some
      - Break on [DONE] or parse returning None with no content
      - Convert errors to ProviderError
- Handle HTTP status codes:
  - 401 -> ProviderError::InvalidApiKey
  - 429 -> ProviderError::RateLimited (parse Retry-After header if present, default 60)
  - 5xx -> ProviderError::Unavailable
  - Other errors -> ProviderError::RequestFailed

Update crates/core/src/provider/mod.rs:
- Add `pub mod openai;` and `pub mod sse;`
- Re-export OpenAiProvider

Update crates/core/src/lib.rs:
- Add re-export for OpenAiProvider
  </action>
  <verify>
- cargo check -p cherry2k-core compiles
- cargo clippy -p cherry2k-core -- -D warnings passes
  </verify>
  <done>OpenAI provider implements AiProvider trait with SSE streaming</done>
</task>

<task type="auto">
  <name>Task 2: Create Ctrl+C handler with confirmation</name>
  <files>
    crates/cli/Cargo.toml
    crates/cli/src/signal.rs
    crates/cli/src/main.rs
  </files>
  <action>
Add to crates/cli/Cargo.toml dependencies:
- tokio-util.workspace = true (for CancellationToken)

Create crates/cli/src/signal.rs:
- Import tokio::signal, tokio_util::sync::CancellationToken
- Implement setup_cancellation() -> CancellationToken:
  - Create CancellationToken
  - Clone token
  - Spawn task that:
    - Awaits signal::ctrl_c()
    - Prints "\n\nCancel response? [y/n]: "
    - Uses tokio::task::spawn_blocking to read stdin line
    - If input.trim() starts with 'y' or 'Y', cancel the token
    - Otherwise print "Continuing..." and loop back to await ctrl_c again
  - Return token
- Add doc comments explaining the confirmation behavior

Update crates/cli/src/main.rs:
- Add `pub mod signal;`
  </action>
  <verify>cargo check -p cherry2k compiles</verify>
  <done>Ctrl+C handler with y/n confirmation created</done>
</task>

<task type="auto">
  <name>Task 3: Integrate streaming into chat command</name>
  <files>
    crates/cli/src/commands/chat.rs
    crates/cli/src/main.rs
  </files>
  <action>
Update crates/cli/src/commands/chat.rs:
- Import cherry2k_core::{OpenAiProvider, AiProvider, CompletionRequest, Message, Role}
- Import crate::output::{ResponseSpinner, StreamWriter, display_error, render_markdown}
- Import crate::signal::setup_cancellation
- Import tokio_stream::StreamExt
- Import anyhow::Context

Rewrite run() function:
```rust
pub async fn run(config: &Config, message: &str, plain: bool) -> Result<()> {
    // Get OpenAI config or error
    let openai_config = config.openai.clone()
        .ok_or_else(|| anyhow::anyhow!("OpenAI not configured. Set OPENAI_API_KEY environment variable."))?;

    // Create and validate provider
    let provider = OpenAiProvider::new(openai_config);
    provider.validate_config()
        .map_err(|e| anyhow::anyhow!("{}", e))?;

    // Build request
    let request = CompletionRequest::new()
        .message(Role::User, message);

    // Setup cancellation
    let cancel_token = setup_cancellation();

    // Show spinner
    let spinner = ResponseSpinner::new();
    spinner.start();

    // Get stream
    let stream = match provider.complete(request).await {
        Ok(s) => s,
        Err(e) => {
            spinner.stop();
            display_error(&e);
            return Err(e.into());
        }
    };

    // Stop spinner, print prefix
    spinner.stop();
    println!();  // Blank line before response
    print!("▶ ");  // Subtle prefix

    // Stream response with cancellation support
    let mut writer = StreamWriter::new();
    let mut stream = stream;
    tokio::pin!(stream);

    loop {
        tokio::select! {
            chunk = stream.next() => {
                match chunk {
                    Some(Ok(text)) => {
                        writer.write_chunk(&text)?;
                    }
                    Some(Err(e)) => {
                        writer.flush()?;
                        println!();
                        display_error(&e);
                        return Err(e.into());
                    }
                    None => break,
                }
            }
            _ = cancel_token.cancelled() => {
                writer.flush()?;
                println!("\n\nCancelled by user.");
                return Ok(());
            }
        }
    }

    writer.flush()?;

    // Render markdown if not plain mode
    // (For now, just print as-is since we stream incrementally)
    // Markdown rendering would need to buffer full response

    println!();  // Blank line after response

    Ok(())
}
```

Update crates/cli/src/main.rs:
- Add --plain/-p flag to Chat command struct
- Pass plain flag to chat::run()

Remove the old demonstration code (demonstrate_confirmation_flow).
  </action>
  <verify>
- cargo check --workspace compiles
- cargo clippy --workspace -- -D warnings passes
- cargo test --workspace passes
  </verify>
  <done>Chat command streams AI responses with spinner, error handling, and cancellation</done>
</task>

</tasks>

<verification>
Run these commands to verify the plan is complete:

```bash
# All code compiles
cargo check --workspace

# Clippy passes
cargo clippy --workspace -- -D warnings

# Tests pass
cargo test --workspace

# Manual testing (requires OPENAI_API_KEY):
# export OPENAI_API_KEY=sk-...
# cargo run -- chat "What is Rust in one sentence?"
# Should see spinner, then streaming response

# Test error handling:
# OPENAI_API_KEY=invalid cargo run -- chat "test"
# Should see boxed error about invalid API key

# Test cancellation:
# cargo run -- chat "Tell me a long story"
# Press Ctrl+C during response
# Should see "Cancel response? [y/n]:" prompt
```
</verification>

<success_criteria>
1. cargo check --workspace passes with no errors
2. `cherry2k chat "prompt"` with valid API key shows spinner then streams response
3. Response is line-buffered (lines appear as they complete, not character-by-character)
4. Invalid API key shows boxed error with config guidance
5. Network errors show boxed error with retry suggestion
6. Ctrl+C prompts for confirmation before cancelling
7. --plain flag is available (markdown rendering deferred to future enhancement)
</success_criteria>

<output>
After completion, create `.planning/phases/02-single-provider-e2e/02-03-SUMMARY.md`
</output>
