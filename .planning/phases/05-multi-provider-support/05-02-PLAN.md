---
phase: 05-multi-provider-support
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - crates/core/src/provider/ollama.rs
  - crates/core/src/provider/mod.rs
autonomous: true

must_haves:
  truths:
    - "Ollama provider compiles and passes validate_config()"
    - "Ollama provider streams responses via NDJSON parsing"
    - "Ollama not running returns clear Unavailable error with hint"
    - "Ollama health_check() verifies connectivity"
  artifacts:
    - path: "crates/core/src/provider/ollama.rs"
      provides: "Ollama provider implementation"
      exports: ["OllamaProvider"]
      min_lines: 150
  key_links:
    - from: "crates/core/src/provider/ollama.rs"
      to: "AiProvider trait"
      via: "impl AiProvider for OllamaProvider"
      pattern: "impl AiProvider for OllamaProvider"
---

<objective>
Implement Ollama local inference provider with NDJSON streaming support.

Purpose: Enable users to run local LLMs through Ollama, fulfilling PROV-03 requirement.
Output: Working `OllamaProvider` that implements `AiProvider` trait with NDJSON streaming.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-provider-support/05-RESEARCH.md

# Existing provider implementation to follow
@crates/core/src/provider/trait.rs
@crates/core/src/provider/openai.rs
@crates/core/src/provider/types.rs
@crates/core/src/config/types.rs
@crates/core/src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create OllamaProvider implementation</name>
  <files>crates/core/src/provider/ollama.rs</files>
  <action>
Create `OllamaProvider` struct implementing `AiProvider` trait.

**Structure:**
```rust
pub struct OllamaProvider {
    client: Client,
    config: OllamaConfig,
}
```

**Key implementation details:**

1. **API Endpoint:** `{host}/api/chat` (default host: `http://localhost:11434`)

2. **No Authentication:** Ollama doesn't require API keys (local service)

3. **Request Body Format:**
   ```json
   {
     "model": "llama3.2",
     "messages": [{"role": "user", "content": "..."}],
     "stream": true
   }
   ```

4. **CRITICAL - NDJSON Streaming (NOT SSE):**
   Ollama uses newline-delimited JSON, not Server-Sent Events.

   Each line in the response is a complete JSON object:
   ```json
   {"model":"llama3.2","message":{"role":"assistant","content":"Hello"},"done":false}
   {"model":"llama3.2","message":{"role":"assistant","content":" there"},"done":false}
   {"model":"llama3.2","message":{"role":"assistant","content":"!"},"done":true}
   ```

   DO NOT use `reqwest_eventsource`. Instead:
   - Use `response.bytes_stream()` to get raw byte chunks
   - Buffer bytes until newline (`\n`) is found
   - Parse complete lines as JSON
   - Extract `message.content` field
   - Check `done: true` to end stream

   **Buffer handling is critical:**
   - Network chunks don't align with JSON line boundaries
   - Accumulate bytes in `Vec<u8>` buffer
   - When newline found, drain buffer up to newline, parse that line
   - Keep remainder for next iteration

5. **`complete()` method:**
   ```rust
   async move {
       let url = format!("{}/api/chat", self.config.host);
       let response = client.post(&url).json(&body).send().await?;

       if !response.status().is_success() {
           // Handle error status
       }

       let stream = parse_ollama_ndjson_stream(response);
       Ok(Box::pin(stream) as CompletionStream)
   }
   ```

6. **`provider_id()`:** Return `"ollama"`

7. **`validate_config()`:**
   - Ollama doesn't need API key, but host must be valid URL
   - Check host is non-empty
   - Could validate URL format if desired (optional)
   - Return Ok(()) if host is present

8. **`health_check()`:**
   - Hit `{host}/api/version` endpoint (lightweight check)
   - If connection refused: Return `ProviderError::Unavailable` with message:
     `"Ollama not running. Start with: ollama serve"`
   - If response is success: Ok(())
   - Use `reqwest::Error::is_connect()` to detect connection refused

**Error handling:**
- Connection refused -> `ProviderError::Unavailable { provider: "ollama", reason: "Ollama not running. Start with: ollama serve" }`
- Model not found (likely 404) -> `ProviderError::RequestFailed("Model not found: {model}")`
- Other errors -> `ProviderError::RequestFailed`

**NDJSON parsing helper function:**
```rust
fn parse_ollama_ndjson_stream(
    response: reqwest::Response,
) -> impl Stream<Item = Result<String, ProviderError>> {
    try_stream! {
        let mut buffer = Vec::new();
        let mut stream = response.bytes_stream();

        while let Some(chunk) = stream.next().await {
            let chunk = chunk.map_err(|e| ProviderError::StreamInterrupted(e.to_string()))?;
            buffer.extend_from_slice(&chunk);

            // Process complete lines
            while let Some(newline_pos) = buffer.iter().position(|&b| b == b'\n') {
                let line: Vec<u8> = buffer.drain(..=newline_pos).collect();
                let line_str = String::from_utf8_lossy(&line[..line.len()-1]);

                if line_str.trim().is_empty() {
                    continue;
                }

                let json: serde_json::Value = serde_json::from_str(&line_str)
                    .map_err(|e| ProviderError::ParseError(e.to_string()))?;

                if let Some(content) = json["message"]["content"].as_str() {
                    if !content.is_empty() {
                        yield content.to_string();
                    }
                }

                if json["done"].as_bool() == Some(true) {
                    return;
                }
            }
        }
    }
}
```

**Tests (in #[cfg(test)] mod tests):**
- `valid_config_passes()` - default config works
- `provider_id_returns_ollama()` - returns "ollama"
- `provider_is_send_sync()` - trait bounds satisfied

Note: Testing actual streaming requires Ollama running, so unit tests focus on config validation and trait bounds.
  </action>
  <verify>
```bash
cargo check -p cherry2k-core 2>&1 | head -20
cargo test -p cherry2k-core ollama 2>&1 | tail -20
```
  </verify>
  <done>OllamaProvider compiles, implements AiProvider trait, all unit tests pass</done>
</task>

<task type="auto">
  <name>Task 2: Export OllamaProvider from provider module</name>
  <files>crates/core/src/provider/mod.rs</files>
  <action>
Update `crates/core/src/provider/mod.rs` to:

1. Add module declaration: `mod ollama;`
2. Add public re-export: `pub use ollama::OllamaProvider;`

Place these alongside the existing openai and anthropic module declarations.
  </action>
  <verify>
```bash
cargo check -p cherry2k-core 2>&1 | head -10
```
  </verify>
  <done>OllamaProvider is exported from cherry2k_core::provider module</done>
</task>

</tasks>

<verification>
```bash
# Full check
cargo check -p cherry2k-core
cargo clippy -p cherry2k-core -- -D warnings
cargo test -p cherry2k-core ollama

# Verify export
cargo doc -p cherry2k-core --no-deps 2>&1 | grep -i ollama
```
</verification>

<success_criteria>
- OllamaProvider implements AiProvider trait
- NDJSON streaming correctly buffers and parses chunks
- Connection refused gives helpful error message
- All unit tests pass
- No clippy warnings
- Provider is exported from crate root
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-provider-support/05-02-SUMMARY.md`
</output>
